{"cells":[{"cell_type":"markdown","metadata":{"id":"JY62wDuFJXb3"},"source":["**Task:** Implementacja perceptronu (Multi Layer Perceptron, MLP) w Pythonie przy użyciu NumPy\n","\n","\n","Celem tego ćwiczenia jest zrozumienie podstaw działania perceptronu – podstawowej komórki najprostszej sztucznej sieci neuronowej. Zaimplementowany model powinien być funkcjonalny i sprawdzony na zadaniu klasyfikacji dla zbioru danych Iris."]},{"cell_type":"markdown","metadata":{"id":"01GKTT0xFd7b"},"source":["### Trochę teorii"]},{"cell_type":"markdown","metadata":{"id":"64_L0xe6Fd7c"},"source":["\n","**Perceptron**\n","\n","Perceptron to model klasyfikatora liniowego, który podejmuje decyzje na podstawie liniowej kombinacji przykładów wejściowych sieci i wag. Perceptron składa się z:\n","\n","- Wektorów wejściowych x=[x1,x2,…,xn]x=[x1​,x2​,…,xn​],\n","- Wektorów wag w=[w1,w2,…,wn]w=[w1​,w2​,…,wn​],\n","- Biasu b,\n","- Funkcji aktywacji (liniowa, relu, sigmoid)\n","\n","Model matematycznie można zapisać jako:\n","`y=f(z),z=w.T*x+b`\n","\n","gdzie:\n","- y to wyjście perceptronu,\n","- f(z) to funkcja aktywacji sigmoid, zdefiniowana:\n","\n","      f(z)=e^z / (1 + e^z)\n","\n","**Algorytm uczenia perceptronu**\n","\n","Uczenie perceptronu polega na dostosowaniu wag w i biasu b na podstawie błędu klasyfikacji (np. błędu średniokwadratowego, MSE). Aktualizacja wag odbywa się według reguły (gradient i pochodne liczone w poprzednim labie):\n","\n","    wi←wi+η(ytrue−ypred)f'(z)xi\n","    b←b+η(ytrue−ypred)f'(z)\n","\n","gdzie:\n","\n","- η to współczynnik uczenia (np. 0.1),\n","- ytrue​ to rzeczywista etykieta klasy,\n","- ypred​ to przewidywany target (etykieta klasy).\n","\n","Proces powtarza się iteracyjnie dla wszystkich przykładów w zbiorze treningowym aż do osiągnięcia kryterium stopu (zbieżności lub wykonania maksymalnej liczby epok).\n","\n","\n","**NOTE:** Z datasetu iris wybieramy tylko 2 klasy i 2 cechy, żeby implementacja i interpretacja była łatwiejsza.\n","\n","($) Zapisanie prostej sieci dla zbioru Iris i 3 klas.\n","\n","($) Wyznaczenie binary cross entropy jako loss function."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xtCfbqYBFd7c"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","# Wczytanie zbioru Iris\n","iris = datasets.load_iris()\n","X = iris.data[:, [2, 3]]  # Wybieramy dwie cechy: długość i szerokość płatka\n","y = iris.target\n","\n","# Wybieramy tylko klasy \"setosa\" i \"versicolor\"\n","X = X[y != 2]\n","y = y[y != 2]\n","\n","# Zamiana etykiet na {0, 1}\n","\n","# Podział na zbiór treningowy i testowy\n","\n","# Standaryzacja danych\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QJYSZZLjFd7e"},"outputs":[],"source":["class Perceptron:\n","\n","    def __init__(self, learning_rate=0.01, epochs=1000):\n","        # Powinno zawierac:\n","        # learning_rate\n","        # epochs\n","        # wieghts\n","        # bias\n","\n","    def activation(self, x):\n","        # Implementacja funkcji aktywacji relu\n","\n","    def fit(self, X, y):\n","        # implementacja funkcji uczenia\n","        # inicjalizacja wag i biasu\n","\n","        # uczenie przez n epok i po wszystkich danych uczacych\n","        # wyjscie sieci\n","        # predykcje\n","        # policzenie gradientu\n","        # updatu wag\n","        # update biasu\n","\n","    def predict(self, X):\n","        # predykcje z uzyciem modelu (neuron i funkcja aktywacji)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gP4aDBciFd7f"},"outputs":[],"source":["# Inicjalizacja i trenowanie perceptronu\n","\n","# Predykcja na zbiorze testowym\n","\n","# Obliczenie dokładności modelu\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GWYbFD57Fd7f"},"outputs":[],"source":["def plot_decision_boundary(X, y, model):\n","    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n","    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n","    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n","\n","    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n","    Z = Z.reshape(xx.shape)\n","\n","    plt.contourf(xx, yy, Z, alpha=0.3)\n","    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors=\"k\", cmap=plt.cm.Paired)\n","    plt.xlabel(\"Długość płatka\")\n","    plt.ylabel(\"Szerokość płatka\")\n","    plt.title(\"Granica decyzyjna perceptronu\")\n","    plt.show()\n","\n","plot_decision_boundary(X_train, y_train, perceptron)\n"]}],"metadata":{"colab":{"provenance":[{"file_id":"1oVNFB42r41uOq8OFm59_darR-FwGJXtp","timestamp":1741638491313},{"file_id":"1i2ZrjlohFUzF9JnYpwxgqIVrL0QoxoBC","timestamp":1741638447495},{"file_id":"1BNMeN4ITYo0jqTKHBSEebTmCCNAsX9Li","timestamp":1741533505309}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}