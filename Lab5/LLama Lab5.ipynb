{"cells":[{"cell_type":"markdown","source":["**TASK** Recap:\n","\n","- Jak działa perceptron?\n","- Jaka jest podstawowa operacja podczas propagacji informacji w przód w Sieci Feed Forward?\n","- Po co są funkcje aktywacji w sieciach neuronowych?\n","- Jakie są ograniczenia uczenia SSN metodą Stochastycznego Spadku wzdłuż gradientu?\n","- Co to są warsty Dropout i BatchNormalization i po co się je stosuje w sieciach neuronowych?\n","- Czym charakteryzują się neuronowe sieci konwolucyjne?\n","- Co to jest operacja splotu?\n","- Czym charakteryzują się neuronowe sieci rekurencyjne?\n","- Jakie istnieją metody poprawy skuteczności modeli rekurencyjnych w przetwarzaniu języka naturalnego?"],"metadata":{"id":"fsrq30vVkhvp"}},{"cell_type":"markdown","source":[],"metadata":{"id":"8sOajSbjPl17"}},{"cell_type":"markdown","metadata":{"id":"EFwSaNB8jF7s"},"source":["<style>\n","td {\n","  text-align: center;\n","}\n","\n","th {\n","  text-align: center;\n","}\n","</style>"]},{"cell_type":"markdown","source":["## LLAMA\n","\n","**NOTE** Ten notebook zadziała tylko na środowisku z GPU T4."],"metadata":{"id":"rIkTVQSXls-W"}},{"cell_type":"code","source":["!pip install transformers accelerate bitsandbytes"],"metadata":{"id":"Mn6cl28Lll2W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load the Llama model locally - might take a few sec."],"metadata":{"id":"VJWV2Y_0JrHT"}},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","model_name = \"meta-llama/Llama-3.2-3B-Instruct\" # \"meta-llama/Llama-2-7b-chat-hf\"\n","prompt = \"Tell me a joke about artificial neural networks\"\n","access_token = \"PUT_YOUR_APIKEY_HERE\"\n","\n","\n","\n","model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", load_in_4bit=True,  use_auth_token=access_token)\n","tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, use_auth_token=access_token)"],"metadata":{"id":"twWR9yPalySo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**NOTE** Knowing how much memory is needed to store a 3B-LLM (above cell run), how much memory is needed for Gemini/GPT with ~200B parameters?"],"metadata":{"id":"4nS6-iDBNgci"}},{"cell_type":"markdown","source":["Test the model"],"metadata":{"id":"v7StHww6Juia"}},{"cell_type":"code","source":["model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda:0\")\n","\n","output = model.generate(**model_inputs)"],"metadata":{"id":"BpfpwUZvJon0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(output[0])"],"metadata":{"id":"e-yWm74j0LQp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(tokenizer.decode(output[0], skip_special_tokens=True))"],"metadata":{"id":"P-kY2sP70LrK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**TASK** Perform entity extraction using Llama model on the below text. Extract the following information (keys) as JSON key value pairs:\n","- PERSON\n","- ORGANIZATION\n","- LOCATION\n","- DATE\n","- MONEY"],"metadata":{"id":"b0MUUCZGJ0Vz"}},{"cell_type":"code","source":["prompt_enity = \"\"\"\n","On March 15, 2023, OpenAI announced a new partnership with Microsoft during a press conference held in San Francisco, California. The collaboration aims to integrate advanced AI models into Microsoft’s Azure cloud platform, with a projected investment of over $1 billion.\n","According to Sam Altman, CEO of OpenAI, the partnership will accelerate the democratization of artificial intelligence across industries. In a related development, Google DeepMind unveiled a new version of its Gemini model on the same day in London.\n","Meanwhile, the European Commission is preparing new AI regulations, expected to be finalized by Q4 of 2024. These regulations could affect companies like Amazon, Meta, and IBM, which are actively developing generative AI applications.\n","Analysts from Goldman Sachs and McKinsey & Company predict that AI could contribute up to $15.7 trillion to the global economy by 2030, with the healthcare and finance sectors likely to benefit the most.\n","\"\"\"\n"],"metadata":{"id":"rkLeHhe40Luq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**TIP** For this inference a transformer pipeline might be more suitable - **restart kernel** before execution."],"metadata":{"id":"TdbRfiLkOpOq"}},{"cell_type":"code","source":["import torch\n","from transformers import pipeline"],"metadata":{"id":"EzNYej-m0Lx0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n","pipe = pipeline(\n","    \"text-generation\",\n","    model=model_id,\n","    torch_dtype=torch.bfloat16,\n","    device_map=\"auto\",\n",")"],"metadata":{"id":"UMbN4MXTO1vm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["messages = [\n","    {\"role\": \"system\", \"content\": \"MODEL_EXPERTISE\"},\n","    {\"role\": \"user\", \"content\": \"USER_PROMPT\"},\n","]"],"metadata":{"id":"rHMLkUX6VEXg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["outputs = pipe(\n","    messages,\n","    max_new_tokens=256,\n",")\n","print(outputs[0][\"generated_text\"][-1])"],"metadata":{"id":"StoSLs7oO2Rm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(outputs[0][\"generated_text\"][-1][\"content\"])"],"metadata":{"id":"nwiYyFFmPTc2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**TASK** Test the Llama model on some coding tasks with your prompt:\n","- Write a Python function that computes the nth Fibonacci number using recursion. Include basic input validation and a docstring.\n","- Write a Python function that returns the first non-repeating character in a string. If all characters repeat, return None.\n","- The coding task of your choice\n","\n","**TIP** You might want to test models fine tuned for code (if you have access to them): https://huggingface.co/meta-llama"],"metadata":{"id":"bNpi3IeWKnpE"}},{"cell_type":"code","source":["coding_prompt = \"\"\"\n","PUT_YOUR_PROMPT_HERE\n","\"\"\""],"metadata":{"id":"HI3x-q970L3w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**TASK** ($$) Build a simple chatbot class using local Llama model:\n","- start with a simple chatbot base prompt, that specifies LLM expertise\n","- pass user question to a chatbot by adding the user question to the base prompt (inference through a method `invoke`)\n","- return a string answer to the user (tokenize LLM output)\n","- ($) keep history of last 3 conversations in the chatbot, by putting them into the prompt. If the conversation is to long, prepare a summary of each message and put the summaries into the LLM prompt"],"metadata":{"id":"MEo0iAlQL9rK"}},{"cell_type":"code","source":[],"metadata":{"id":"Zr1XEZ-plyVd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To learn more about transformers and attention mechanism - the very engine of LLMs - you might:\n","- read original article on attention mechanism (~ 175k cites): https://arxiv.org/abs/1706.03762\n","- watch a brilliant explanatory video on transformers: https://www.youtube.com/watch?v=KJtZARuO3JY&ab_channel=GrantSanderson\n","- practice coding transformers from scratch with tensorflow: https://www.tensorflow.org/text/tutorials/transformer?hl=pl  "],"metadata":{"id":"3yr8SNofTBeq"}}],"metadata":{"colab":{"last_runtime":{"build_target":"//learning/deepmind/public/tools/ml_python:ml_notebook","kind":"private"},"private_outputs":true,"provenance":[{"file_id":"1kw1632J-k9kGR1AdrDvvkAE6plzsO1wL","timestamp":1744373905122},{"file_id":"1U8RoTkLloMOVxLoXIsho1w5CMkFZ1Z-f","timestamp":1744373437277},{"file_id":"1cuoBSpWc0UNkLgMitqREBS0iGimAr-wd","timestamp":1743934636034},{"file_id":"https://github.com/tensorflow/text/blob/master/docs/tutorials/transformer.ipynb","timestamp":1743934021914}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}