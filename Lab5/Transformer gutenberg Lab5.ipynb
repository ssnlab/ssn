{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyP9+GI4QcCFpm0i2qj0zl3S"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Generacja tekstu z użyciem prostego transformera"],"metadata":{"id":"JRZZgOVYeXUT"}},{"cell_type":"code","source":["! pip install keras_nlp"],"metadata":{"id":"jY6FjsrIQttp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import keras_nlp\n","import numpy as np\n","import random\n","from tensorflow.keras.layers import TextVectorization"],"metadata":{"id":"y5NBbd0DeivA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Wczytanie danych do zmiennych"],"metadata":{"id":"wPXl6hBVlt1c"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"XE3yEtq2dzaF"},"outputs":[],"source":["crime_and_punishment_url = 'https://www.gutenberg.org/files/2554/2554-0.txt'\n","brothers_of_karamazov_url = 'https://www.gutenberg.org/files/28054/28054-0.txt'\n","the_idiot_url = 'https://www.gutenberg.org/files/2638/2638-0.txt'\n","\n","paths = [crime_and_punishment_url, brothers_of_karamazov_url, the_idiot_url]\n","names = ['Crime and Punishment', 'Brothers of Karamazov', 'The Idiot']\n","texts = ''\n","for index, path in enumerate(paths):\n","    filepath = keras.utils.get_file(f'{names[index]}.txt', origin=path)\n","    text = ''\n","    with open(filepath, encoding='utf-8') as f:\n","        text = f.read()\n","        # First 50 lines are the Gutenberg intro and preface\n","        # Skipping first 10k characters for each book should be approximately\n","        # removing the intros and prefaces.\n","        texts += text[10000:]"]},{"cell_type":"code","source":["# przykładowe 500 znaków\n","texts[25000:25500]"],"metadata":{"id":"t7bomqHXRAIc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text_list = texts.split('.')\n","len(text_list) # 50835 - estymacja ilości zdań"],"metadata":{"id":"FSDaMLXWRBzP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Podział danych na treningowe, testowe i walidacyjne"],"metadata":{"id":"fdgmSGi-68D1"}},{"cell_type":"code","source":["random.shuffle(text_list)\n","length = len(text_list)\n","text_train = text_list[:int(0.7*length)]\n","text_test = text_list[int(0.7*length):int(0.85*length)]\n","text_valid = text_list[int(0.85*length):]"],"metadata":{"id":"RHheBLWaREPW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Definicja tworzenia sekwencji i vectorizera"],"metadata":{"id":"RZ0ezRqjl0Yk"}},{"cell_type":"code","source":["len(max(text_list).split(' '))"],"metadata":{"id":"u6EJSTBL7Lba"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def custom_standardization(input_string):\n","    sentence = tf.strings.lower(input_string)\n","    sentence = tf.strings.regex_replace(sentence, \"\\n\", \" \")\n","    return sentence\n","\n","# określenie długości zdania\n","maxlen = len(max(text_list).split(' ')) # 25\n","\n","vectorize_layer = TextVectorization(\n","    standardize = custom_standardization,\n","    output_mode=\"int\",\n","    output_sequence_length=maxlen+1,\n",")\n","\n","vectorize_layer.adapt(text_list)\n","vocab = vectorize_layer.get_vocabulary()"],"metadata":{"id":"YLJO_w0FlzEq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Ustalenie wielkości słownika"],"metadata":{"id":"mak5p-c67TU8"}},{"cell_type":"code","source":["vocab_size = len(vocab)\n","vocab_size # 40677"],"metadata":{"id":"ak4VzHu0eelT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["index_lookup = dict(zip(range(len(vocab)), vocab))\n","index_lookup[5] # of"],"metadata":{"id":"D2JaC7PMRSLB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Przykładowy wektor reprezentujący zdanie"],"metadata":{"id":"HN14WZ00SDpN"}},{"cell_type":"code","source":["v = vectorize_layer(['The final goal of life is...'])\n","print(v, '\\n', len(v[0]))"],"metadata":{"id":"Yj8O4gclRTD9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Stworzenie sekwencji wejściowych z podziałem na zbiór treningowy, walidacyjny i testowy"],"metadata":{"id":"1FpR1VmZl-Rg"}},{"cell_type":"code","source":["batch_size = 64\n","\n","train_dataset = tf.data.Dataset.from_tensor_slices(text_train)\n","train_dataset = train_dataset.shuffle(buffer_size=256)\n","train_dataset = train_dataset.batch(batch_size)\n","\n","test_dataset = tf.data.Dataset.from_tensor_slices(text_test)\n","test_dataset = test_dataset.shuffle(buffer_size=256)\n","test_dataset = test_dataset.batch(batch_size)\n","\n","valid_dataset = tf.data.Dataset.from_tensor_slices(text_valid)\n","valid_dataset = valid_dataset.shuffle(buffer_size=256)\n","valid_dataset = valid_dataset.batch(batch_size)"],"metadata":{"id":"nwj5YKxoRTS9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Przygotowanie danych do treningu wraz z optymalizacją przetwarzania kolejnych batchy z przez równoległe pobieranie z wyprzedzeniem w Tensorflow (https://www.tensorflow.org/guide/data_performance?hl=pl#prefetching)"],"metadata":{"id":"x63OJE2o8A7M"}},{"cell_type":"code","source":["def preprocess_text(text):\n","    text = tf.expand_dims(text, -1)\n","    tokenized_sentences = vectorize_layer(text)\n","    x = tokenized_sentences[:, :-1]\n","    y = tokenized_sentences[:, 1:]\n","    return x, y\n","\n","\n","train_dataset = train_dataset.map(preprocess_text)\n","train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n","\n","test_dataset = test_dataset.map(preprocess_text)\n","test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)\n","\n","valid_dataset = valid_dataset.map(preprocess_text)\n","valid_dataset = valid_dataset.prefetch(tf.data.AUTOTUNE)"],"metadata":{"id":"6-4lElFlRTXP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Przykład przeprocesowanego tekstu"],"metadata":{"id":"vBN3ds2USPnx"}},{"cell_type":"code","source":["for entry in train_dataset.take(1):\n","    print(entry)"],"metadata":{"id":"gW4hISzbRTaa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Definicja Transformera z keras nlp"],"metadata":{"id":"is4hLOKveYyt"}},{"cell_type":"code","source":["embed_dim = 128\n","num_heads = 4\n","\n","def create_model():\n","    inputs = keras.layers.Input(shape=(maxlen,), dtype=tf.int32)\n","    embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(vocab_size, maxlen, embed_dim)(inputs)\n","    decoder = keras_nlp.layers.TransformerDecoder(intermediate_dim=embed_dim,\n","                                                            num_heads=num_heads,\n","                                                            dropout=0.5)(embedding_layer)\n","\n","    outputs = keras.layers.Dense(vocab_size, activation='softmax')(decoder)\n","\n","    model = keras.Model(inputs=inputs, outputs=outputs)\n","\n","    model.compile(\n","        optimizer=\"adam\",\n","        loss='sparse_categorical_crossentropy',\n","        metrics=[keras_nlp.metrics.Perplexity(), 'accuracy']\n","    )\n","    return model\n","\n","model = create_model()\n","model.summary()"],"metadata":{"id":"kbjKP4IxRhfY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TextSampler(keras.callbacks.Callback):\n","    def __init__(self, start_prompt, max_tokens):\n","        self.start_prompt = start_prompt\n","        self.max_tokens = max_tokens\n","\n","    # Próbkowanie kolejnego tokenu z 5 tokenów transformera z najwiekszym PDP\n","    def sample_token(self, logits):\n","        logits, indices = tf.math.top_k(logits, k=5, sorted=True)\n","        indices = np.asarray(indices).astype(\"int32\")\n","        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n","        preds = np.asarray(preds).astype(\"float32\")\n","        return np.random.choice(indices, p=preds)\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        decoded_sample = self.start_prompt\n","\n","        for i in range(self.max_tokens-1):\n","            tokenized_prompt = vectorize_layer([decoded_sample])[:, :-1]\n","            predictions = self.model.predict([tokenized_prompt], verbose=0)\n","            # To find the index of the next word in the prediction array.\n","            # The tokenized prompt is already shorter than the original decoded sample\n","            # by one, len(decoded_sample.split()) is two words\n","            # (shorter lenght and and len is always +1 than index ) ahead -\n","            # so we remove 1 to get the next word in the sequence\n","            sample_index = len(decoded_sample.strip().split())-1\n","\n","            sampled_token = self.sample_token(predictions[0][sample_index])\n","            sampled_token = index_lookup[sampled_token]\n","            decoded_sample += \" \" + sampled_token\n","\n","        print(f\"\\nSample text:\\n{decoded_sample}...\\n\")\n","\n","# First 5 words of a random sentence to be used as a seed\n","random_sentence = ' '.join(random.choice(text_valid).replace('\\n', ' ').split(' ')[:4])\n","sampler = TextSampler(random_sentence, maxlen)\n","reducelr = keras.callbacks.ReduceLROnPlateau(patience=10, monitor='val_loss')"],"metadata":{"id":"VIPikb_QRiZv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Trening modelu"],"metadata":{"id":"_LHg-iARmdtL"}},{"cell_type":"code","source":["model = create_model()\n","history = model.fit(train_dataset,\n","                    validation_data=valid_dataset,\n","                    epochs=5,\n","                    callbacks=[sampler, reducelr])"],"metadata":{"id":"JvHXy4Yxerqg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Funkcja pomocnicza do generacji tekstu"],"metadata":{"id":"UQWMmlDseZds"}},{"cell_type":"code","source":["def sample_token(logits):\n","        logits, indices = tf.math.top_k(logits, k=5, sorted=True)\n","        indices = np.asarray(indices).astype(\"int32\")\n","        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n","        preds = np.asarray(preds).astype(\"float32\")\n","        return np.random.choice(indices, p=preds)\n","\n","def generate_text(prompt, response_length=20):\n","    decoded_sample = prompt\n","    for i in range(response_length-1):\n","        tokenized_prompt = vectorize_layer([decoded_sample])[:, :-1]\n","        predictions = model.predict([tokenized_prompt], verbose=0)\n","        sample_index = len(decoded_sample.strip().split())-1\n","\n","        sampled_token = sample_token(predictions[0][sample_index])\n","        sampled_token = index_lookup[sampled_token]\n","        decoded_sample += \" \" + sampled_token\n","    return decoded_sample"],"metadata":{"id":"HuG-J-AoeWoa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Przykład generacji tekstu z użyciem transformera i sekwencji początkowej"],"metadata":{"id":"bZL082KRepJx"}},{"cell_type":"code","source":["generate_text(\"Last summer, \")\n"],"metadata":{"id":"MXzqeFmseWrC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Aiukp9wLeWwd"},"execution_count":null,"outputs":[]}]}