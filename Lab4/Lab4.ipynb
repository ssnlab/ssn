{"cells":[{"cell_type":"markdown","metadata":{"id":"xtgW5D5thaHw"},"source":["RECAP\n","\n","Sieci Rekurencyjne posiadają pamięć o sekwencjach w danych wejściowych poprzez modeyfikację standardowej komórki perceptronu. To czyni je skuteczne w przetwarzaniu języka:\n","\n","- skuteczność dla danych sekwencyjnych ze względu na pamięć w komórkach sieci\n","\n","- komórki LSTM i GRU mają (ograniczoną) zdolność przetwarzania odległych zależności w sekwencji\n","\n","- te dwie własności pozwalają na proste rozumienie kontekstu przez RNN, w szczególności LSTM lub GRU\n","\n","SimpleRNN: prostsze i mają krótki kontekst\n","\n","LSTM: bardziej skomplikowana komórka pamięci i możliwość skutecznego modelowania dłuższego kontekstu (dłuższe sekwencje)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4x_i-efnNU_E"},"outputs":[],"source":["import sys\n","import numpy as np\n","import tensorflow as tf\n","import copy\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.datasets import imdb\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, Flatten, Dense, Dropout, GRU\n","from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n","from transformers import pipeline\n","from sklearn.metrics import accuracy_score\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from tensorflow.keras.utils import to_categorical"]},{"cell_type":"markdown","metadata":{"id":"73xUo3T1h8JX"},"source":["### Stemming, Lemmatyzacja - procesowanie tekstu\n","\n","**Stemming** usuwa końcówki fleksyjne, sprowadzając słowo do rdzenia (np. running → run). Jest szybki, ale mniej precyzyjny.\n","\n","\n","**Lemmatyzacja** używa słowników językowych, aby znaleźć poprawną formę podstawową (np. better → good). Jest dokładniejsza, ale wolniejsza.\n","\n","Metody bywają przydatne w podnoszeniu dokładności modeli tekstowych do różnych zadań."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XefZRq0yoPM9"},"outputs":[],"source":["import nltk\n","from nltk.stem import PorterStemmer, WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","\n","# Pobieranie potrzebnych pakietów\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('punkt_tab')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fov1h0cCNN_N"},"outputs":[],"source":["# Przykładowy tekst\n","text = \"We love programming on a piece of paper.\"\n","\n","# Tokenizacja wyrazów\n","words = word_tokenize(text)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OzxoutQ0NPqL"},"outputs":[],"source":["# Inicjalizacja Stemmera i Lemmatizera\n","stemmer = PorterStemmer()\n","lemmatizer = WordNetLemmatizer()\n","\n","# Stemming wyrazów\n","stemmed_words = [stemmer.stem(word) for word in words]\n","\n","# Lematyzacja wyrazów\n","lemmatized_words = [lemmatizer.lemmatize(word) for word in words]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EOxDIzOqNRK5"},"outputs":[],"source":["print(\"Original Words:    \", words)\n","print(\"Stemmed Words:     \", stemmed_words)\n","print(\"Lemmatized Words:  \", lemmatized_words)"]},{"cell_type":"markdown","metadata":{"id":"HzkcHssROgoz"},"source":["**TASK** Przetestować powyższe metody (stemming, lemmatzyacja) na wymyślonym przez siebie zdaniu."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HUdA85OiNU_I"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"3RG2RvNZh5il"},"source":["### Reprezentacja tekstu w Sieci Neuronowej"]},{"cell_type":"markdown","metadata":{"id":"-qFdw6B7NU_J"},"source":["Podział zdań na wyrazy - tokenizacja"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sTBJbbjyoXHE"},"outputs":[],"source":["# Tokenizacja tekstu\n","text = \"Never have I ever complained about SSN classes\"\n","tokens = text.split(\" \")\n","tokens"]},{"cell_type":"markdown","metadata":{"id":"r0Eg0QaeNU_K"},"source":["Kodowanie wyrazów na cyfry dla całego słownika - wszystkie słowa w zbiorze danych"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"37dtdGxBoXJm"},"outputs":[],"source":["# Proste enkodowanie numeryczne\n","token_to_index = {word: idx + 1 for idx, word in enumerate(tokens)}\n","encoded_text = list(token_to_index.values())"]},{"cell_type":"markdown","metadata":{"id":"QCSBv3wiADuP"},"source":["Do przetwarzania zakodowanych słów przez sieć neuronową, wektory wejściowe (zwykle) muszą mieć ten sam rozmiar. W związku z tym, po kodowaniu wykonuje się przycinanie lub dodawanie augmentowanych wartości do próbek wejściowych.\n","\n","**TASK** W poniższej komórce uzupełnić wartość `maxlen` funkcji `pad_sequences` na długość pierwszego wektora wejściowego z listy (`sequences`). Opisać co dzieje się podczas paddingu dla różnych wektorów wejściowych z tej listy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PFSokZ_LoXNA"},"outputs":[],"source":["# Padding - wypełnianie zaenkodowanych sekwencji żeby były jednej długości\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","sequences = [[1, 2, 3, 4, 5, 6, 7, 8],[1, 1, 1, 2, 3, 4, 5, 6, 7, 8],  [1, 2, 3, 4, 5, 6]]\n","padded_sequences = pad_sequences(sequences, maxlen=<PUT_VALUE_HERE>, padding=\"post\")\n","print(padded_sequences)\n"]},{"cell_type":"markdown","metadata":{"id":"6ZsE9VmHh8MQ"},"source":["### Bag of words model\n","\n","\n","Model Bag of Words (BoW) to jedna z najprostszych metod reprezentacji tekstu w formie numerycznej. W BoW każdy dokument jest zamieniany na wektor o wymiarach odpowiadających liczbie unikalnych słów w korpusie, gdzie wartości reprezentują liczbę wystąpień danego słowa w dokumencie. Model ten ignoruje kolejność słów i zależności między nimi, co sprawia, że nie uchwyca kontekstu, ale jest efektywny i często stosowany w klasyfikacji tekstu oraz wyszukiwaniu informacji. Udoskonaloną wersją BoW jest podejście TF-IDF, które uwzględnia częstotliwość słów w dokumentach, aby lepiej różnicować istotne terminy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YXnkB7t9iEua"},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Przykładowe zdania\n","corpus = [\n","    \"I love AGH\",\n","    \"The longer I think about it the less I understand it\",\n","    \"It is better to walk slowly in a good diirection than run around a false hope\"\n","]\n","\n","# Inicjalizacja COunt Vectorizera\n","vectorizer = CountVectorizer()\n","\n","# Fit transform\n","X = vectorizer.fit_transform(corpus)\n","\n","print(\"Nazwy cech\", vectorizer.get_feature_names_out())\n","print(\"Reprezentacja BoW:\\n\", X.toarray())\n"]},{"cell_type":"markdown","metadata":{"id":"Jpqxi3_nNU_M"},"source":["**TASK** Wypróbować powyższe modele na zdaniu \"The longer I study at AGH the less I understand it, but I trust the process\". Wytłumaczyć dlaczego wektor BoW przyjął takie wartości - interpretacja działania modelu."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QK86_HQzNU_M"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"g2qXnm61PTjo"},"source":["###  TF-IDF\n","\n","TF-IDF (Term Frequency-Inverse Document Frequency) to miara wykorzystywana w przetwarzaniu języka naturalnego, która służy do oceny ważności słów w dokumencie w kontekście zbioru dokumentów.\n","\n","Składa się z dwóch głównych komponentów:\n","\n","- Term Frequency (TF): Jest to miara częstotliwości wystąpienia danego terminu (słowa) w dokumencie. Oblicza się ją jako stosunek liczby wystąpień słowa do całkowitej liczby słów w dokumencie:\n","  - liczba wystąpień terminu t w dokumencie, dzielona przez\n","  - całkowita liczba słów w dokumencie\n","\n","- Inverse Document Frequency (IDF): Jest to miara rzadkości danego terminu w całym zbiorze dokumentów. Oblicza się ją na podstawie liczby dokumentów, w których występuje dany termin. Jeśli termin występuje w wielu dokumentach, to jego waga będzie niższa. IDF oblicza się jako logarytm odwrotności proporcji dokumentów zawierających dany termin do całkowitej liczby dokumentów:\n","   - IDF(t)=log⁡(N/df(t))\n","\n","    Gdzie:\n","    - N to całkowita liczba dokumentów,\n","\n","    - df(t) to liczba dokumentów zawierających termin t.\n","\n","Finalnie, TF-IDF to po prostu iloczyn obu tych miar:\n","\n","TF-IDF(t)=TF(t)×IDF(t)\n","\n","Celem TF-IDF jest nadanie większej wagi tym słowom, które są istotne w danym dokumencie, ale rzadko występują w innych dokumentach w zbiorze. Dzięki temu można lepiej ocenić, które terminy najlepiej opisują treść dokumentu, a które są mniej istotne lub powszechne."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BJ2-ODcmp9s9"},"outputs":[],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","\n","# Przykładowe dokumenty\n","corpus = [\n","    \"Zagrać pauzą, ciszą, akcentem, zmianą rytmu mówienia. Tego wciąż nikt nie uczy\",\n","    \"Fizycznej gotówki mamy nieprzebrane ilości\",\n","    \"Płacimy dobrze, ale wymagamy też bardzo dużo\",\n","    \"Nie da się przeszczepić organu od martwego dawcy\"\n","]\n","\n","# Tworzenie modelu TF-IDF\n","vectorizer = TfidfVectorizer()\n","X = vectorizer.fit_transform(corpus)\n","\n","# Wypisanie wyników\n","print(\"Słowa w słowniku:\", vectorizer.get_feature_names_out())\n","print(\"Macierz TF-IDF:\\n\", X.toarray())\n"]},{"cell_type":"markdown","metadata":{"id":"Fo41phvXlv70"},"source":["Tradycyjne metody, takie jak kodowanie numeryczne, Bag of Words (BoW) czy TF-IDF, reprezentują tekst w sposób numeryczny, ale słąbo uwzględniają znaczenia słów ani ich kontekstu. Słowa, które są synonimami lub mają podobne znaczenie, w tych podejściach są traktowane jako niezależne jednostki.\n","\n","**Word embeddings** to reprezentacje wektorowe słów, kodując ich znaczenie w przestrzeni numerycznej. Są to gęste, ciągłe wektory o stałej długości, które pozwalają na uchwycenie semantycznych i syntaktycznych relacji między słowami.\n","Podczas używania Word Embeddings, słowa o podobnym znaczeniu znajdują się blisko siebie w przestrzeni wektorowej. Modele Sieci Neuronowych uczą się mapować słowa na wektory poprzez analizę ich kontekstu w dużych zbiorach danych tekstowych, np. `Word2Vec` lub transformery."]},{"cell_type":"markdown","metadata":{"id":"W3L5dBuOHBXw"},"source":["\n","Przykładowe architektury modeli Word Embedding:\n","\n","CBOW (Continuous Bag of Words): Model przwiduje słowo na podstawie podanych wyrazów z otoczenia przewidywanego słowa.\n","\n","Example:\n","\n","- Input: [\"The\", \"longer\", \"I\", \"about\", \"it\"]\n","- Label: \"think\"\n","\n","Skip-gram: Model przewiduje sąsiednie wyrazy dla wybranego słowa wejściowego.\n","\n","Example:\n","- Input: \"think\"\n","- Label: [\"The\", \"longer\", \"I\", \"about\", \"it\"]\n","\n","Poniżej znajduje się przykładowa implementacja modelu Word2Vec w Keras. Dane są augmentowane dla ułatwienia implementacji."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tW-9rZxGCaC_"},"outputs":[],"source":["# Defninicja danych testowych\n","corpus = [\n","    \"I love AGH\",\n","    \"The longer I think about it the less I understand it\",\n","    \"It is better to walk slowly in a good direction than run around a false hope\"\n","]\n","\n","input = [\n","    \"I love\",\n","    \"The longer I think about it the I understand it\",\n","    \"It is better to walk slowly in a good direction than run around a hope\"\n","]\n","\n","label = [\n","    \"AGH\",\n","    \"less\",\n","    \"false\"\n","]\n","\n","# Przygotowanie danych do modelu\n","\n","# Tokenizacja\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(corpus)\n","\n","# Definicja rozmiaru wektora wejsciwego - ilość słów w słowniku\n","vocab_size = len(tokenizer.word_index) + 1\n","\n","# Przygotowanie sekwencji\n","train_sequences = tokenizer.texts_to_sequences(input)\n","train_padded_sequences = np.array(pad_sequences(train_sequences, maxlen=vocab_size, padding='post'))\n","\n","# Przygotowanie sekwencji\n","label_sequences = np.array(tokenizer.texts_to_sequences(label))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NwX9mfPDI_Dr"},"outputs":[],"source":["print(train_padded_sequences)\n","print(label_sequences)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cBrr7gUREpb4"},"outputs":[],"source":["# Definicja modelu WordEmbedding\n","embedding_dim = 8  # Define embedding size\n","model = Sequential([\n","    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=train_padded_sequences.shape[1]),\n","    Flatten(),\n","    Dense(8, activation='relu'),\n","    Dense(vocab_size, activation='softmax')\n","])\n","\n","# Kompilacja modelu\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2ZMtIudLFDWr"},"outputs":[],"source":["# Trening modelu na zaugmentowanych danych\n","model.fit(train_padded_sequences, label_sequences.flatten(), epochs=10, verbose=1)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LbnrcGCDEqMp"},"outputs":[],"source":["# Ekstrakcja embeddingów dla poszczególnych słow modelu\n","embedding_layer = model.layers[0]\n","embedding_matrix = embedding_layer.get_weights()[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PJLeHpvYFKgn"},"outputs":[],"source":["embedding_matrix.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8W577CaKEr0Q"},"outputs":[],"source":["# Wyciągnięcie embeddingów dla poszczególnych słow\n","word_embeddings = {word: embedding_matrix[idx] for word, idx in tokenizer.word_index.items()}\n","\n","# Przykład użycia\n","word = \"love\"\n","if word in word_embeddings:\n","    print(f\"Vector for word '{word}':\")\n","    print(word_embeddings[word])\n","else:\n","    print(f\"Word '{word}' not in vocabulary\")"]},{"cell_type":"markdown","metadata":{"id":"M40oyZNzOtco"},"source":["**TASK** Przetestować powyższy model enkodujący na wybranym przez siebie zdaniu. Należy zdefiniować zdanie, wykonać tokenizacje, padding i wywołać predykcje modelu. Opisać dlaczego predykcja modelu ma taki rozmiar."]},{"cell_type":"code","source":[],"metadata":{"id":"xBMVpemmQIRJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JEEa-Ub-3aze"},"source":["### Analiza sentymentu z Siecią LSTM dla zbiory IMBD film reviews\n","\n","**TIP** Warto zmienić środowisko programistyczne na środowisko z *GPU*."]},{"cell_type":"markdown","metadata":{"id":"9gX74vITcWyI"},"source":["Przygotowanie danych do treningu"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vdxb5Sp3g1f6"},"outputs":[],"source":["# Load IMDB dataset\n","max_features = 10000\n","max_len = 200\n","(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n","x_train = pad_sequences(x_train, maxlen=max_len)\n","x_test = pad_sequences(x_test, maxlen=max_len)\n"]},{"cell_type":"markdown","metadata":{"id":"TASnX6w1caz8"},"source":["**TASK** Zdefiniować sieć Simple RNN. Dodać warsty: `Embedding` i `SimpleRNN`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zq2xrgTbg1mB"},"outputs":[],"source":["rnn_model = Sequential([\n","    <PLACEHOLDER_FOR_EMBEDDING_LAYER>,\n","    <PLACEHOLDER_FOR_SIMPLERNN_LAYER>,\n","    Dense(1, activation='sigmoid')\n","])\n","\n","rnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5TQfQxjKg1NW"},"outputs":[],"source":["# Trening i ewaluacja modelu SimpleRNN\n","rnn_model.fit(x_train, y_train, epochs=3, batch_size=64, validation_data=(x_test, y_test))\n","rnn_acc = rnn_model.evaluate(x_test, y_test, verbose=0)[1]"]},{"cell_type":"markdown","metadata":{"id":"ma0vhDfcVaTB"},"source":["**TASK** Zdefiniowac sieć LSTM do predykcji sentymentu zbioru danych IMBD. Wytrenować model LSTM podobnie jak model SimpleRNN powyżej. Wyznaczyć metrykę accuracy z treningu."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gFEpd55Pg1qC"},"outputs":[],"source":["lstm_model = Sequential()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1ue9FaX7ZNas"},"outputs":[],"source":["# Trening i ewaluacja modelu LSTM\n","lstm_model.fit(x_train, y_train, epochs=3, batch_size=64, validation_data=(x_test, y_test))\n","lstm_acc = lstm_model.evaluate(x_test, y_test, verbose=0)[1]"]},{"cell_type":"markdown","metadata":{"id":"jfIbwzyIcluB"},"source":["Przygotowanie tekstu z tokenów dla porównania dokładności klasyfikacji RNN z pretrenowanym transformerem."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tzJRTdv1g--B"},"outputs":[],"source":["# Pobranie i stworzenie mapy dla zbioru danych imbd\n","word_index = imdb.get_word_index()\n","\n","# Odwrocenie word_index na tekst\n","index_word = {v + 3: k for k, v in word_index.items()}  # Offset by 3 (IMDB special tokens)\n","index_word[0], index_word[1], index_word[2], index_word[3] = \"<PAD>\", \"<START>\", \"<UNK>\", \"<UNUSED>\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V-e70d3cg1Ok"},"outputs":[],"source":["def decode_review(encoded_review):\n","    return \" \".join([index_word.get(i, \"?\") for i in encoded_review])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SU5fBcNphId8"},"outputs":[],"source":["# Dekodowanie tokenow\n","x_test_texts = [decode_review(seq) for seq in x_test]"]},{"cell_type":"markdown","metadata":{"id":"19bhN2dQbSNz"},"source":["Przykładowy zdekodowany tekst"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZOP4F11qmGnk"},"outputs":[],"source":["x_test_texts[1]"]},{"cell_type":"markdown","metadata":{"id":"c1qXVm8MbV7c"},"source":["Ładowanie transformera z biblioteki hugging face"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6xTm2JSchIzN"},"outputs":[],"source":["sentiment_pipeline = pipeline(\"sentiment-analysis\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UF-p_fhpn1gu"},"outputs":[],"source":["# Przykład predykcji\n","sentiment_pipeline(x_test_texts[1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PZ1ObS34cwbB"},"outputs":[],"source":["# Mapa do etykiet dla transformera\n","map_label = {\"NEGATIVE\": 0, \"POSITIVE\": 1}"]},{"cell_type":"markdown","metadata":{"id":"O24d34iicxD9"},"source":["Predykcje z użyciem transformera.\n","\n","**NOTE**: Predykcje mogą chwile potrwać."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"htdfb5pUYB28"},"outputs":[],"source":["pred_transformer = sentiment_pipeline(x_test_texts, batch_size=100, truncation=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uqd-vcJ2Xtla"},"outputs":[],"source":["pred_trans_mapped = [map_label[item[\"label\"]] for item in pred_transformer]"]},{"cell_type":"markdown","metadata":{"id":"UVqYt8CfYTYl"},"source":["Obliczenie accuracy dla transformera"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mvzPutlVXYVv"},"outputs":[],"source":["transformer_acc = accuracy_score(y_test, pred_trans_mapped)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kja_6IeNA3Zk"},"outputs":[],"source":["# Comparison results\n","print(f\"RNN Accuracy: {rnn_acc:.4f}\")\n","print(f\"LSTM Accuracy: {lstm_acc:.4f}\")\n","print(f\"Transformer Accuracy: {transformer_acc:.4f}\")\n"]},{"cell_type":"markdown","metadata":{"id":"aZBJFh7gXI2y"},"source":["**TASK** Wykonać predykcję dla 1000 losowych danych z zbioru danych IMBD z użyciem SimpleRNN, LSTM i transformera (`sentiment_pipeline`). Wyznaczyć metryki accuracy, precision i recall dla każdego modelu."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MLvcv-n7NU_b"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"lk8zBokrNU_b"},"source":["**TASK** Podzielić dataset IMBD na dane treningowe, walidacyjne i testowe. Powtórzyć definicję i trening modeli SimpleRNN i LSTM. Wytrenować oba modele RNN na danych treningowych i użyć danych walidacyjnych do celów walidacji podczas treningu. Następnie porównać dokładność modeli SimpleRNN, LSTM i transformer (`sentiment_pipeline`) na danych testowych, które nie były używane do treningu modelu ani walidacji podczas treningu."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BnTHNAfVNU_b"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"jUO_IEn499_6"},"source":["## Generacja tekstu\n","\n","**NOTE** Należy dodać plik `wonderland.txt` z folderu Lab4 w repozytorium do plików aktywnego środowiska Google Colab."]},{"cell_type":"markdown","metadata":{"id":"_fKt4ZMgr5Ed"},"source":["Ładowanie danych"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PJTmEc22r5HD"},"outputs":[],"source":["filename = \"wonderland.txt\"\n","raw_text = open(filename, 'r', encoding='utf-8').read()\n","raw_text = raw_text.lower()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bYYEQcrhr5Jg"},"outputs":[],"source":["# Stworzenie mapy znaków do int\n","chars = sorted(list(set(raw_text)))\n","char_to_int = dict((c, i) for i, c in enumerate(chars))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u-DI4fTOr5MA"},"outputs":[],"source":["n_chars = len(raw_text)\n","n_vocab = len(chars)\n","print(\"Total Characters: \", n_chars)\n","print(\"Total Vocab: \", n_vocab)"]},{"cell_type":"markdown","source":["Przygotowanie danych wejściowych do sieci:\n","- podział danych na X i y, które reprezentują tekst i jego kontynuację, której generowanie będziemy trenować\n","- zmiana znaków tekstowych na liczby"],"metadata":{"id":"FQVJrVtXV5qv"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"eslhsKhOr5OY"},"outputs":[],"source":["seq_length = 100\n","x_enc = []\n","y_enc = []\n","for i in range(0, n_chars - seq_length, 1):\n","    seq_in = raw_text[i:i + seq_length]\n","    seq_out = raw_text[i + seq_length]\n","    x_enc.append([char_to_int[char] for char in seq_in])\n","    y_enc.append(char_to_int[seq_out])\n","n_patterns = len(x_enc)\n","print(\"Total Patterns: \", n_patterns)"]},{"cell_type":"code","source":["x_enc[3][:5]"],"metadata":{"id":"zmGSWJ_aWM44"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_enc[3]"],"metadata":{"id":"94eg3sKYWSD_"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QoSf4cFKr5Qy"},"outputs":[],"source":["# reshape X do sieci [samples, time steps, features]\n","X = np.reshape(x_enc, (n_patterns, seq_length, 1))\n","# Normalizacja danych\n","X = X / float(n_vocab)\n","# kodowanie targetu\n","y = to_categorical(y_enc)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J8XBQiNyr5Tc"},"outputs":[],"source":["# Definicja modelu\n","model = Sequential([\n","    LSTM(128, input_shape=(X.shape[1], X.shape[2])),\n","    Dropout(0.1),\n","    Dense(y.shape[1], activation='softmax')\n","])\n","\n","model.compile(loss='categorical_crossentropy', optimizer='adam')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LNMAoP3iujLq"},"outputs":[],"source":["# stworzenie mapy int to char - odwrócenie działania modelu\n","int_to_char = dict((i, c) for i, c in enumerate(chars))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VfiZ3x9-r5V7"},"outputs":[],"source":["# definicja checkpointu modelu\n","filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.weights.h5\"\n","checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min', save_weights_only=True)\n","callbacks_list = [checkpoint]"]},{"cell_type":"markdown","source":["**TIP** Przy wielokrotnym trenowaniu modelu można wyłączyć tworzenie callbacku, żeby nie powielać zapisywania modeli."],"metadata":{"id":"z6B7UXB5XJgW"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"8G0LSrbrr5Ym"},"outputs":[],"source":["model.fit(X, y, epochs=5, batch_size=128, callbacks=callbacks_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qOryrZbjr5eY"},"outputs":[],"source":["# generacja tekstu z losowym początkiem\n","start = np.random.randint(0, len(x_enc)-1)\n","pattern = x_enc[start]\n","start_pattern = copy.copy(pattern)\n","print(\"Seed:\")\n","print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RXhB1L5kNU_e"},"outputs":[],"source":["# generacja tekstu przez n iteracji\n","n = 25\n","generated_text = []\n","\n","for i in range(n):\n","    x = np.reshape(pattern, (1, len(pattern), 1))\n","    # normalizacja\n","    x = x / float(n_vocab)\n","    prediction = model.predict(x, verbose=0)\n","    # Wybranie następnego znaku zgodnie z największym prawdopodobieństwem zwróconym przez model\n","    index = np.argmax(prediction)\n","    # konwersja predykcji do znaku\n","    result = int_to_char[index]\n","    pattern.append(index)\n","    generated_text.append(result)\n","    # aktualizacja danych wejściowych o wygenerowany znak - podobnie jak w predykcji dla szeregów czasowych\n","    pattern = pattern[1:len(pattern)]"]},{"cell_type":"markdown","source":["Wygenerowany text"],"metadata":{"id":"HF7DARdFZIVb"}},{"cell_type":"code","source":["print((\"\").join([item for item in generated_text]))"],"metadata":{"id":"jaMWk5d2ZIpQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Połączony wygenerowany tekst z wzorem początkowym"],"metadata":{"id":"oWTmF7mDZI_9"}},{"cell_type":"code","source":["start_pattern_text = [int_to_char[p] for p in start_pattern]\n","print((\"\").join([item for item in [*start_pattern_text, *generated_text]]))"],"metadata":{"id":"GDpJ2UiwXk1_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jhzUE0gEsiEa"},"source":["**TASK** Próba poprawy generowanego tekstu (jako, że nie mamy etykiet, miara poprawy jest bardziej analityczna niż numeryczna):\n","- zwiększyć model LSTM (więcej warstw, dropout itd.)\n","- zamienić komórki LSTM na GRU (https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRU)\n","- usprawnić parametry treningu: rozmiar batcha, ilość epok\n","- zmienić ilość przewidywanych znaków przez sieć\n","- zmienić ilość słow wejściowych w sieci\n","- usunąć niepotrzebne znaki z tekstu (text-processing)\n","- użyć paddingu do tworzenia sekwencji\n","- zmienić sposób wybierania następnego znaku przez model. Początkowo jest znak z największym prawdopodobieństwem. Można to zmienić na ważone losowe dobieranie jednego z `n` znaków z największym prawdopodobieństwem z predykcji.\n","- na koniec sprawdzić i podać ilość parametrów trenowanlych w modelu"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HU3DEmIGvJu1"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"EZppjGIEuxKa"},"source":["**TASK** ($$$) Zaimplementować klasę z komórką LSTM wg. oryginalnej implementacji z publikacji: https://doi.org/10.1162/neco.1997.9.8.1735. Pomocnicze materiały: (https://www.geeksforgeeks.org/deep-learning-introduction-to-long-short-term-memory/, https://colah.github.io/posts/2015-08-Understanding-LSTMs/). Komórka powinna być zaimplentowana tylko do propagowania informacji w przód i przetestowana na sztucznie wygenerowanych wartościach wag i próbek."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1-tuqTh_r5hG"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.4"}},"nbformat":4,"nbformat_minor":0}